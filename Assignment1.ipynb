{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c86af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code is adapted from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "#The following function generates a sample from a probability distribution. You may choose to ignore the logic. Just see how to use it.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "def weighted_choice(v, p):\n",
    "   total = sum(p)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in zip(v,p):\n",
    "      if upto + w >= r:\n",
    "         return c\n",
    "      upto += w\n",
    "   assert False, \"Shouldn't get here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb7056c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the above function to sample from a distribution\n",
    "\n",
    "#6-faced die with equal probabilities\n",
    "Sample_Space=[1,2,3,4,5,6]\n",
    "Prob_Values=[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "\n",
    "#Generating a sample\n",
    "weighted_choice(Sample_Space, Prob_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aebf473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class for MDP environment definition that takes Transition Probability Matrix p(s'|s,a) and reward E[R_{t+1} | s,a,s'] as inputs\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possible states \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = random.choice(tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
    "        next_state = weighted_choice(possible_states, p=probs)\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                                              \"but is instead %s\" % (\n",
    "                                                              state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action], dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                                                          \"a dictionary but is instead %s\" % (\n",
    "                                                                              state, action,\n",
    "                                                                              type(transition_probs[state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                                    \"add up to %f (should be 1)\" % (state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                                                                 \"a dictionary but is instead %s\" % (\n",
    "                                                                 state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db3c16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a MDP shown in this Figure https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\n",
    "\n",
    "transition_probs = {'s0':{'a0': {'s0': 0.5, 's2': 0.5},'a1': {'s2': 1}},\n",
    "                    's1':{'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "                    's2':{'a0': {'s0': 0.4, 's1': 0.6},'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}}\n",
    "                   }\n",
    "rewards = {'s1': {'a0': {'s0': +5}},'s2': {'a1': {'s0': -1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d69e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize an environment\n",
    "env=MDP(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d26571e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at s1\n"
     ]
    }
   ],
   "source": [
    "#To know the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f977e38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s1', 0.0, False, {})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To take an action and observe the next state, reward, 'whether terminal state reached or not', any other description.\n",
    "env.step('a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "831c3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {'s0':{'A_U': {'s0': 0.9, 's1': 0.1}, 'A_D': {'s4': 0.8, 's0': 0.1, 's1': 0.1}, 'A_L': {'s0': 0.9, 's4': 0.1}, 'A_R': {'s1': 0.8, 's0': 0.1, 's4': 0.1}},\n",
    "                    's1':{'A_U': {'s1': 0.8, 's2': 0.1, 's0': 0.1}, 'A_D': {'s1': 0.8, 's2': 0.1, 's0': 0.1},   'A_L': {'s0': 0.8, 's1': 0.2}, 'A_R': {'s2': 0.8, 's1': 0.2}},\n",
    "                    's2':{'A_U': {'s2': 0.8, 's3': 0.1, 's1': 0.1}, 'A_D': {'s6': 0.8, 's1': 0.1, 's3': 0.1}, 'A_L': {'s1': 0.8, 's6': 0.1, 's2': 0.1}, 'A_R': {'s3': 0.8, 's2': 0.1, 's6': 0.1}},\n",
    "                    's3':{},\n",
    "                    's4':{'A_U': {'s0': 0.8, 's4': 0.2}, 'A_D': {'s8': 0.8, 's4': 0.2}, 'A_L': {'s4': 0.8, 's8': 0.1, 's0': 0.1}, 'A_R': {'s4': 0.8, 's8': 0.1, 's0': 0.1}},\n",
    "                    's5':{},\n",
    "                    's6':{'A_U': {'s2': 0.8, 's6': 0.1, 's7': 0.1}, 'A_D': {'s10': 0.8, 's6': 0.1, 's7': 0.1}, 'A_L': {'s6': 0.8, 's2': 0.1, 's10': 0.1}, 'A_R': {'s7': 0.8, 's10': 0.1, 's2': 0.1}},\n",
    "                    's7':{},\n",
    "                    's8':{'A_U': {'s4': 0.8, 's8': 0.1, 's9': 0.1}, 'A_D': {'s8': 0.9, 's9': 0.1}, 'A_L': {'s8': 0.9, 's4': 0.1}, 'A_R': {'s9': 0.8, 's8': 0.1, 's4': 0.1}},\n",
    "                    's9':{'A_U': {'s9': 0.8, 's8': 0.1, 's10': 0.1}, 'A_D': {'s9': 0.8, 's8': 0.1, 's10': 0.1}, 'A_L': {'s8': 0.8, 's9': 0.2}, 'A_R': {'s10': 0.8, 's9': 0.2}},\n",
    "                    's10':{'A_U': {'s6': 0.8, 's9': 0.1, 's11': 0.1}, 'A_D': {'s10': 0.8, 's9': 0.1, 's11': 0.1}, 'A_L': {'s9': 0.8, 's10': 0.1, 's6': 0.1}, 'A_R': {'s11': 0.8, 's10': 0.1, 's6': 0.1}},\n",
    "                    's11':{'A_U': {'s7': 0.8, 's10': 0.1, 's11': 0.1}, 'A_D': {'s11': 0.9, 's10': 0.1}, 'A_L': {'s10': 0.8, 's11': 0.1, 's7': 0.1}, 'A_R': {'s11': 0.9, 's7': 0.1}}\n",
    "                    }\n",
    "\n",
    "rewards = {'s0':{'A_U': {'s0': -0.01, 's1': -0.01}, 'A_D': {'s4': -0.01, 's0': -0.01, 's1': -0.01}, 'A_L': {'s0': -0.01, 's4': -0.01}, 'A_R': {'s1': -0.01, 's0': -0.01, 's4': -0.01}},\n",
    "            's1':{'A_U': {'s1': -0.01, 's2': -0.01, 's0': -0.01}, 'A_D': {'s1': -0.01, 's2': -0.01, 's0': -0.01}, 'A_L': {'s0': -0.01, 's1': -0.01}, 'A_R': {'s2': -0.01, 's1': -0.01}},\n",
    "            's2':{'A_U': {'s2': -0.01, 's3': 1, 's1': -0.01}, 'A_D': {'s6': -0.01, 's1': -0.01, 's3': 1}, 'A_L': {'s1': -0.01, 's6': -0.01, 's2': -0.01}, 'A_R': {'s3': 1, 's2': -0.01, 's6': -0.01}},\n",
    "            's3':{}, \n",
    "            's4':{'A_U': {'s0': -0.01, 's4': -0.01}, 'A_D': {'s8': -0.01, 's4': -0.01}, 'A_L': {'s4': -0.01, 's8': -0.01, 's0': -0.01}, 'A_R': {'s4': -0.01, 's8': -0.01, 's0': -0.01}},\n",
    "            's5':{},\n",
    "            's6':{'A_U': {'s2': -0.01, 's6': -0.01, 's7': -1}, 'A_D': {'s10': -0.01, 's6': -0.01, 's7': -1}, 'A_L': {'s6': -0.01, 's2': -0.01, 's10': -0.01}, 'A_R': {'s7': -1, 's10': -0.01, 's2': -0.01}},\n",
    "            's7':{},\n",
    "            's8':{'A_U': {'s4': -0.01, 's8': -0.01, 's9': -0.01}, 'A_D': {'s8': -0.01, 's9': -0.01}, 'A_L': {'s8': -0.01, 's4': -0.01}, 'A_R': {'s9': -0.01, 's8': -0.01, 's4': -0.01}},\n",
    "            's9':{'A_U': {'s9': -0.01, 's8': -0.01, 's10': -0.01}, 'A_D': {'s9': -0.01, 's8': -0.01, 's10': -0.01}, 'A_L': {'s8': -0.01, 's9': -0.01}, 'A_R': {'s10': -0.01, 's9': -0.01}},\n",
    "            's10':{'A_U': {'s6': -0.01, 's9': -0.01, 's11': -0.01}, 'A_D': {'s10': -0.01, 's9': -0.01, 's11': -0.01}, 'A_L': {'s9': -0.01, 's10': -0.01, 's6': -0.01}, 'A_R': {'s11': -0.01, 's10': -0.01, 's6': -0.01}},\n",
    "            's11':{'A_U': {'s7': -1, 's10': -0.01, 's11': -0.01}, 'A_D': {'s11': -0.01, 's10': -0.01}, 'A_L': {'s10': -0.01, 's11': -0.01, 's7': -1}, 'A_R': {'s11': -0.01, 's7': -1}}\n",
    "            }\n",
    "\n",
    "initial_state = 's0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba5b3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1=MDP(transition_probs, rewards, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "649c48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4181d3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A_U', 'A_D', 'A_L', 'A_R')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_possible_actions('s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "302bddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.is_terminal('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73206c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s3': 0.8, 's2': 0.1, 's6': 0.1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_next_states('s2','A_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbee656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_transition_prob('s2','A_R','s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43f6082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_reward('s2','A_R','s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0550ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions=['A_U','A_L','A_D','A_R']\n",
    "Prob_Values=[1/4, 1/4, 1/4, 1/4]\n",
    "\n",
    "#Generating a sample\n",
    "Policy = weighted_choice(Actions, Prob_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0236981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A_D\n",
      "s4 -0.01 False\n",
      "1 A_U\n",
      "s4 -0.01 False\n",
      "2 A_R\n",
      "s0 -0.01 False\n",
      "3 A_R\n",
      "s0 -0.01 False\n",
      "4 A_R\n",
      "s1 -0.01 False\n",
      "5 A_U\n",
      "s2 -0.01 False\n",
      "6 A_L\n",
      "s1 -0.01 False\n",
      "7 A_L\n",
      "s1 -0.01 False\n",
      "8 A_D\n",
      "s1 -0.01 False\n",
      "9 A_R\n",
      "s1 -0.01 False\n",
      "10 A_U\n",
      "s1 -0.01 False\n",
      "11 A_U\n",
      "s0 -0.01 False\n",
      "12 A_D\n",
      "s0 -0.01 False\n",
      "13 A_R\n",
      "s4 -0.01 False\n",
      "14 A_D\n",
      "s8 -0.01 False\n",
      "15 A_R\n",
      "s8 -0.01 False\n",
      "16 A_L\n",
      "s8 -0.01 False\n",
      "17 A_D\n",
      "s4 -0.01 False\n",
      "18 A_D\n",
      "s4 -0.01 False\n",
      "19 A_D\n",
      "s8 -0.01 False\n",
      "20 A_U\n",
      "s4 -0.01 False\n",
      "21 A_D\n",
      "s4 -0.01 False\n",
      "22 A_R\n",
      "s8 -0.01 False\n",
      "23 A_U\n",
      "s4 -0.01 False\n",
      "24 A_U\n",
      "s4 -0.01 False\n",
      "25 A_U\n",
      "s8 -0.01 False\n",
      "26 A_U\n",
      "s9 -0.01 False\n",
      "27 A_R\n",
      "s9 -0.01 False\n",
      "28 A_D\n",
      "s10 -0.01 False\n",
      "29 A_L\n",
      "s11 -0.01 False\n",
      "30 A_U\n",
      "s7 -1 True\n"
     ]
    }
   ],
   "source": [
    "is_done = 0\n",
    "for i in range(100):\n",
    "    if(is_done != 1):\n",
    "        state, reward, is_done,_ = env1.step(weighted_choice(Actions, Prob_Values))\n",
    "        print(i,weighted_choice(Actions, Prob_Values))\n",
    "        print(state,reward,is_done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
