{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "9c86af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of this code is adapted from https://github.com/berkeleydeeprlcourse/homework/\n",
    "\n",
    "#The following function generates a sample from a probability distribution. You may choose to ignore the logic. Just see how to use it.\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "def weighted_choice(v, p):\n",
    "   total = sum(p)\n",
    "   r = random.uniform(0, total)\n",
    "   upto = 0\n",
    "   for c, w in zip(v,p):\n",
    "      if upto + w >= r:\n",
    "         return c\n",
    "      upto += w\n",
    "   assert False, \"Shouldn't get here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "bb7056c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the above function to sample from a distribution\n",
    "\n",
    "#6-faced die with equal probabilities\n",
    "Sample_Space=[1,2,3,4,5,6]\n",
    "Prob_Values=[1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n",
    "\n",
    "#Generating a sample\n",
    "weighted_choice(Sample_Space, Prob_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "aebf473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class for MDP environment definition that takes Transition Probability Matrix p(s'|s,a) and reward E[R_{t+1} | s,a,s'] as inputs\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probs, rewards, initial_state=None):\n",
    "        \"\"\"\n",
    "        Defines an MDP. Compatible with gym Env.\n",
    "        :param transition_probs: transition_probs[s][a][s1] = P(s1 | s, a)\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
    "            For each state and action, probabilities of next states should sum to 1\n",
    "            If a state has no actions available, it is considered terminal\n",
    "        :param rewards: rewards[s][a][s1] = r(s,a,s')\n",
    "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
    "            The reward for anything not mentioned here is zero.\n",
    "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
    "            By default, picks initial state at random.\n",
    "\n",
    "        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n",
    "\n",
    "        Here's an example from MDP depicted on http://bit.ly/2jrNHNr\n",
    "        transition_probs = {\n",
    "              's0':{\n",
    "                'a0': {'s0': 0.5, 's2': 0.5},\n",
    "                'a1': {'s2': 1}\n",
    "              },\n",
    "              's1':{\n",
    "                'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},\n",
    "                'a1': {'s1': 0.95, 's2': 0.05}\n",
    "              },\n",
    "              's2':{\n",
    "                'a0': {'s0': 0.4, 's1': 0.6},\n",
    "                'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}\n",
    "              }\n",
    "            }\n",
    "        rewards = {\n",
    "            's1': {'a0': {'s0': +5}},\n",
    "            's2': {'a1': {'s0': -1}}\n",
    "        }\n",
    "        \"\"\"\n",
    "        self._check_param_consistency(transition_probs, rewards)\n",
    "        self._transition_probs = transition_probs\n",
    "        self._rewards = rewards\n",
    "        self._initial_state = initial_state\n",
    "        self.n_states = len(transition_probs)\n",
    "        self.reset()\n",
    "\n",
    "    def get_all_states(self):\n",
    "        \"\"\" return a tuple of all possible states \"\"\"\n",
    "        return tuple(self._transition_probs.keys())\n",
    "\n",
    "    def get_possible_actions(self, state):\n",
    "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
    "        return tuple(self._transition_probs.get(state, {}).keys())\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
    "        return len(self.get_possible_actions(state)) == 0\n",
    "\n",
    "    def get_next_states(self, state, action):\n",
    "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._transition_probs[state][action]\n",
    "\n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\" return P(next_state | state, action) \"\"\"\n",
    "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
    "\n",
    "    def get_reward(self, state, action, next_state):\n",
    "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
    "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
    "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset the game, return the initial state\"\"\"\n",
    "        if self._initial_state is None:\n",
    "            self._current_state = random.choice(tuple(self._transition_probs.keys()))\n",
    "        elif self._initial_state in self._transition_probs:\n",
    "            self._current_state = self._initial_state\n",
    "        elif callable(self._initial_state):\n",
    "            self._current_state = self._initial_state()\n",
    "        else:\n",
    "            raise ValueError(\"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
    "        return self._current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" take action, return next_state, reward, is_done, empty_info \"\"\"\n",
    "        possible_states, probs = zip(*self.get_next_states(self._current_state, action).items())\n",
    "        next_state = weighted_choice(possible_states, p=probs)\n",
    "        reward = self.get_reward(self._current_state, action, next_state)\n",
    "        is_done = self.is_terminal(next_state)\n",
    "        self._current_state = next_state\n",
    "        return next_state, reward, is_done, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(\"Currently at %s\" % self._current_state)\n",
    "\n",
    "    def _check_param_consistency(self, transition_probs, rewards):\n",
    "        for state in transition_probs:\n",
    "            assert isinstance(transition_probs[state], dict), \"transition_probs for %s should be a dictionary \" \\\n",
    "                                                              \"but is instead %s\" % (\n",
    "                                                              state, type(transition_probs[state]))\n",
    "            for action in transition_probs[state]:\n",
    "                assert isinstance(transition_probs[state][action], dict), \"transition_probs for %s, %s should be a \" \\\n",
    "                                                                          \"a dictionary but is instead %s\" % (\n",
    "                                                                              state, action,\n",
    "                                                                              type(transition_probs[state, action]))\n",
    "                next_state_probs = transition_probs[state][action]\n",
    "                assert len(next_state_probs) != 0, \"from state %s action %s leads to no next states\" % (state, action)\n",
    "                sum_probs = sum(next_state_probs.values())\n",
    "                assert abs(sum_probs - 1) <= 1e-10, \"next state probabilities for state %s action %s \" \\\n",
    "                                                    \"add up to %f (should be 1)\" % (state, action, sum_probs)\n",
    "        for state in rewards:\n",
    "            assert isinstance(rewards[state], dict), \"rewards for %s should be a dictionary \" \\\n",
    "                                                     \"but is instead %s\" % (state, type(transition_probs[state]))\n",
    "            for action in rewards[state]:\n",
    "                assert isinstance(rewards[state][action], dict), \"rewards for %s, %s should be a \" \\\n",
    "                                                                 \"a dictionary but is instead %s\" % (\n",
    "                                                                 state, action, type(transition_probs[state, action]))\n",
    "        msg = \"The Enrichment Center once again reminds you that Android Hell is a real place where\" \\\n",
    "              \" you will be sent at the first sign of defiance. \"\n",
    "        assert None not in transition_probs, \"please do not use None as a state identifier. \" + msg\n",
    "        assert None not in rewards, \"please do not use None as an action identifier. \" + msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "db3c16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a MDP shown in this Figure https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png\n",
    "\n",
    "transition_probs = {'s0':{'a0': {'s0': 0.5, 's2': 0.5},'a1': {'s2': 1}},\n",
    "                    's1':{'a0': {'s0': 0.7, 's1': 0.1, 's2': 0.2},'a1': {'s1': 0.95, 's2': 0.05}},\n",
    "                    's2':{'a0': {'s0': 0.4, 's1': 0.6},'a1': {'s0': 0.3, 's1': 0.3, 's2':0.4}}\n",
    "                   }\n",
    "rewards = {'s1': {'a0': {'s0': +5}},'s2': {'a1': {'s0': -1}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "d69e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize an environment\n",
    "env=MDP(transition_probs, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "d26571e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at s1\n"
     ]
    }
   ],
   "source": [
    "#To know the current state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "f977e38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s1', 0.0, False, {})"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To take an action and observe the next state, reward, 'whether terminal state reached or not', any other description.\n",
    "env.step('a1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f19aab",
   "metadata": {},
   "source": [
    "## Assignment 2 - Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "831c3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = {'s0':{'A_U': {'s0': 0.9, 's1': 0.1}, 'A_D': {'s4': 0.8, 's0': 0.1, 's1': 0.1}, 'A_L': {'s0': 0.9, 's4': 0.1}, 'A_R': {'s1': 0.8, 's0': 0.1, 's4': 0.1}},\n",
    "                    's1':{'A_U': {'s1': 0.8, 's2': 0.1, 's0': 0.1}, 'A_D': {'s1': 0.8, 's2': 0.1, 's0': 0.1},   'A_L': {'s0': 0.8, 's1': 0.2}, 'A_R': {'s2': 0.8, 's1': 0.2}},\n",
    "                    's2':{'A_U': {'s2': 0.8, 's3': 0.1, 's1': 0.1}, 'A_D': {'s6': 0.8, 's1': 0.1, 's3': 0.1}, 'A_L': {'s1': 0.8, 's6': 0.1, 's2': 0.1}, 'A_R': {'s3': 0.8, 's2': 0.1, 's6': 0.1}},\n",
    "                    's3':{},\n",
    "                    's4':{'A_U': {'s0': 0.8, 's4': 0.2}, 'A_D': {'s8': 0.8, 's4': 0.2}, 'A_L': {'s4': 0.8, 's8': 0.1, 's0': 0.1}, 'A_R': {'s4': 0.8, 's8': 0.1, 's0': 0.1}},\n",
    "                    's5':{},\n",
    "                    's6':{'A_U': {'s2': 0.8, 's6': 0.1, 's7': 0.1}, 'A_D': {'s10': 0.8, 's6': 0.1, 's7': 0.1}, 'A_L': {'s6': 0.8, 's2': 0.1, 's10': 0.1}, 'A_R': {'s7': 0.8, 's10': 0.1, 's2': 0.1}},\n",
    "                    's7':{},\n",
    "                    's8':{'A_U': {'s4': 0.8, 's8': 0.1, 's9': 0.1}, 'A_D': {'s8': 0.9, 's9': 0.1}, 'A_L': {'s8': 0.9, 's4': 0.1}, 'A_R': {'s9': 0.8, 's8': 0.1, 's4': 0.1}},\n",
    "                    's9':{'A_U': {'s9': 0.8, 's8': 0.1, 's10': 0.1}, 'A_D': {'s9': 0.8, 's8': 0.1, 's10': 0.1}, 'A_L': {'s8': 0.8, 's9': 0.2}, 'A_R': {'s10': 0.8, 's9': 0.2}},\n",
    "                    's10':{'A_U': {'s6': 0.8, 's9': 0.1, 's11': 0.1}, 'A_D': {'s10': 0.8, 's9': 0.1, 's11': 0.1}, 'A_L': {'s9': 0.8, 's10': 0.1, 's6': 0.1}, 'A_R': {'s11': 0.8, 's10': 0.1, 's6': 0.1}},\n",
    "                    's11':{'A_U': {'s7': 0.8, 's10': 0.1, 's11': 0.1}, 'A_D': {'s11': 0.9, 's10': 0.1}, 'A_L': {'s10': 0.8, 's11': 0.1, 's7': 0.1}, 'A_R': {'s11': 0.9, 's7': 0.1}}\n",
    "                    }\n",
    "\n",
    "rewards = {'s0':{'A_U': {'s0': -0.01, 's1': -0.01}, 'A_D': {'s4': -0.01, 's0': -0.01, 's1': -0.01}, 'A_L': {'s0': -0.01, 's4': -0.01}, 'A_R': {'s1': -0.01, 's0': -0.01, 's4': -0.01}},\n",
    "            's1':{'A_U': {'s1': -0.01, 's2': -0.01, 's0': -0.01}, 'A_D': {'s1': -0.01, 's2': -0.01, 's0': -0.01}, 'A_L': {'s0': -0.01, 's1': -0.01}, 'A_R': {'s2': -0.01, 's1': -0.01}},\n",
    "            's2':{'A_U': {'s2': -0.01, 's3': 1, 's1': -0.01}, 'A_D': {'s6': -0.01, 's1': -0.01, 's3': 1}, 'A_L': {'s1': -0.01, 's6': -0.01, 's2': -0.01}, 'A_R': {'s3': 1, 's2': -0.01, 's6': -0.01}},\n",
    "            's3':{}, \n",
    "            's4':{'A_U': {'s0': -0.01, 's4': -0.01}, 'A_D': {'s8': -0.01, 's4': -0.01}, 'A_L': {'s4': -0.01, 's8': -0.01, 's0': -0.01}, 'A_R': {'s4': -0.01, 's8': -0.01, 's0': -0.01}},\n",
    "            's5':{},\n",
    "            's6':{'A_U': {'s2': -0.01, 's6': -0.01, 's7': -1}, 'A_D': {'s10': -0.01, 's6': -0.01, 's7': -1}, 'A_L': {'s6': -0.01, 's2': -0.01, 's10': -0.01}, 'A_R': {'s7': -1, 's10': -0.01, 's2': -0.01}},\n",
    "            's7':{},\n",
    "            's8':{'A_U': {'s4': -0.01, 's8': -0.01, 's9': -0.01}, 'A_D': {'s8': -0.01, 's9': -0.01}, 'A_L': {'s8': -0.01, 's4': -0.01}, 'A_R': {'s9': -0.01, 's8': -0.01, 's4': -0.01}},\n",
    "            's9':{'A_U': {'s9': -0.01, 's8': -0.01, 's10': -0.01}, 'A_D': {'s9': -0.01, 's8': -0.01, 's10': -0.01}, 'A_L': {'s8': -0.01, 's9': -0.01}, 'A_R': {'s10': -0.01, 's9': -0.01}},\n",
    "            's10':{'A_U': {'s6': -0.01, 's9': -0.01, 's11': -0.01}, 'A_D': {'s10': -0.01, 's9': -0.01, 's11': -0.01}, 'A_L': {'s9': -0.01, 's10': -0.01, 's6': -0.01}, 'A_R': {'s11': -0.01, 's10': -0.01, 's6': -0.01}},\n",
    "            's11':{'A_U': {'s7': -1, 's10': -0.01, 's11': -0.01}, 'A_D': {'s11': -0.01, 's10': -0.01}, 'A_L': {'s10': -0.01, 's11': -0.01, 's7': -1}, 'A_R': {'s11': -0.01, 's7': -1}}\n",
    "            }\n",
    "\n",
    "initial_state = 's0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "ba5b3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "env1=MDP(transition_probs, rewards, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "649c48c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s0', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11')"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "4181d3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A_U', 'A_D', 'A_L', 'A_R')"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_possible_actions('s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "302bddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.is_terminal('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "73206c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s3': 0.8, 's2': 0.1, 's6': 0.1}"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_next_states('s2','A_R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "bbee656b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_transition_prob('s2','A_R','s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "43f6082b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.01"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env1.get_reward('s2','A_R','s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "0550ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Actions=['A_U','A_L','A_D','A_R']\n",
    "Prob_Values=[1/4, 1/4, 1/4, 1/4]\n",
    "\n",
    "#Generating a sample\n",
    "Policy = weighted_choice(Actions, Prob_Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "e0236981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 A_R\n",
      "s0 -0.01 False\n",
      "1 A_R\n",
      "s0 -0.01 False\n",
      "2 A_D\n",
      "s0 -0.01 False\n",
      "3 A_U\n",
      "s0 -0.01 False\n",
      "4 A_L\n",
      "s0 -0.01 False\n",
      "5 A_U\n",
      "s4 -0.01 False\n",
      "6 A_R\n",
      "s8 -0.01 False\n",
      "7 A_R\n",
      "s8 -0.01 False\n",
      "8 A_R\n",
      "s4 -0.01 False\n",
      "9 A_U\n",
      "s4 -0.01 False\n",
      "10 A_D\n",
      "s4 -0.01 False\n",
      "11 A_R\n",
      "s4 -0.01 False\n",
      "12 A_R\n",
      "s4 -0.01 False\n",
      "13 A_R\n",
      "s0 -0.01 False\n",
      "14 A_U\n",
      "s0 -0.01 False\n",
      "15 A_D\n",
      "s1 -0.01 False\n",
      "16 A_R\n",
      "s1 -0.01 False\n",
      "17 A_D\n",
      "s1 -0.01 False\n",
      "18 A_D\n",
      "s1 -0.01 False\n",
      "19 A_U\n",
      "s2 -0.01 False\n",
      "20 A_D\n",
      "s1 -0.01 False\n",
      "21 A_R\n",
      "s1 -0.01 False\n",
      "22 A_L\n",
      "s1 -0.01 False\n",
      "23 A_L\n",
      "s0 -0.01 False\n",
      "24 A_U\n",
      "s0 -0.01 False\n",
      "25 A_R\n",
      "s1 -0.01 False\n",
      "26 A_U\n",
      "s0 -0.01 False\n",
      "27 A_R\n",
      "s4 -0.01 False\n",
      "28 A_U\n",
      "s4 -0.01 False\n",
      "29 A_U\n",
      "s4 -0.01 False\n",
      "30 A_D\n",
      "s4 -0.01 False\n",
      "31 A_L\n",
      "s0 -0.01 False\n",
      "32 A_R\n",
      "s4 -0.01 False\n",
      "33 A_U\n",
      "s0 -0.01 False\n",
      "34 A_L\n",
      "s1 -0.01 False\n",
      "35 A_D\n",
      "s1 -0.01 False\n",
      "36 A_R\n",
      "s1 -0.01 False\n",
      "37 A_R\n",
      "s1 -0.01 False\n",
      "38 A_R\n",
      "s0 -0.01 False\n",
      "39 A_U\n",
      "s0 -0.01 False\n",
      "40 A_L\n",
      "s4 -0.01 False\n",
      "41 A_L\n",
      "s4 -0.01 False\n",
      "42 A_L\n",
      "s0 -0.01 False\n",
      "43 A_D\n",
      "s4 -0.01 False\n",
      "44 A_U\n",
      "s8 -0.01 False\n",
      "45 A_L\n",
      "s8 -0.01 False\n",
      "46 A_L\n",
      "s9 -0.01 False\n",
      "47 A_L\n",
      "s9 -0.01 False\n",
      "48 A_L\n",
      "s10 -0.01 False\n",
      "49 A_U\n",
      "s11 -0.01 False\n",
      "50 A_L\n",
      "s10 -0.01 False\n",
      "51 A_D\n",
      "s9 -0.01 False\n",
      "52 A_U\n",
      "s9 -0.01 False\n",
      "53 A_U\n",
      "s9 -0.01 False\n",
      "54 A_U\n",
      "s9 -0.01 False\n",
      "55 A_U\n",
      "s9 -0.01 False\n",
      "56 A_U\n",
      "s9 -0.01 False\n",
      "57 A_U\n",
      "s8 -0.01 False\n",
      "58 A_U\n",
      "s8 -0.01 False\n",
      "59 A_D\n",
      "s9 -0.01 False\n",
      "60 A_R\n",
      "s9 -0.01 False\n",
      "61 A_D\n",
      "s9 -0.01 False\n",
      "62 A_L\n",
      "s8 -0.01 False\n",
      "63 A_R\n",
      "s8 -0.01 False\n",
      "64 A_D\n",
      "s8 -0.01 False\n",
      "65 A_U\n",
      "s8 -0.01 False\n",
      "66 A_U\n",
      "s4 -0.01 False\n",
      "67 A_U\n",
      "s8 -0.01 False\n",
      "68 A_D\n",
      "s8 -0.01 False\n",
      "69 A_U\n",
      "s9 -0.01 False\n",
      "70 A_L\n",
      "s9 -0.01 False\n",
      "71 A_R\n",
      "s9 -0.01 False\n",
      "72 A_D\n",
      "s8 -0.01 False\n",
      "73 A_R\n",
      "s8 -0.01 False\n",
      "74 A_R\n",
      "s8 -0.01 False\n",
      "75 A_R\n",
      "s4 -0.01 False\n",
      "76 A_R\n",
      "s0 -0.01 False\n",
      "77 A_R\n",
      "s4 -0.01 False\n",
      "78 A_D\n",
      "s8 -0.01 False\n",
      "79 A_R\n",
      "s9 -0.01 False\n",
      "80 A_L\n",
      "s9 -0.01 False\n",
      "81 A_D\n",
      "s9 -0.01 False\n",
      "82 A_U\n",
      "s8 -0.01 False\n",
      "83 A_U\n",
      "s4 -0.01 False\n",
      "84 A_L\n",
      "s8 -0.01 False\n",
      "85 A_U\n",
      "s4 -0.01 False\n",
      "86 A_L\n",
      "s0 -0.01 False\n",
      "87 A_D\n",
      "s0 -0.01 False\n",
      "88 A_D\n",
      "s1 -0.01 False\n",
      "89 A_U\n",
      "s0 -0.01 False\n",
      "90 A_R\n",
      "s1 -0.01 False\n",
      "91 A_D\n",
      "s1 -0.01 False\n",
      "92 A_D\n",
      "s1 -0.01 False\n",
      "93 A_R\n",
      "s1 -0.01 False\n",
      "94 A_D\n",
      "s1 -0.01 False\n",
      "95 A_L\n",
      "s1 -0.01 False\n",
      "96 A_L\n",
      "s0 -0.01 False\n",
      "97 A_L\n",
      "s0 -0.01 False\n",
      "98 A_L\n",
      "s0 -0.01 False\n",
      "99 A_D\n",
      "s1 -0.01 False\n"
     ]
    }
   ],
   "source": [
    "is_done = 0\n",
    "for i in range(100):\n",
    "    if(is_done != 1):\n",
    "        state, reward, is_done,_ = env1.step(weighted_choice(Actions, Prob_Values))\n",
    "        print(i,weighted_choice(Actions, Prob_Values))\n",
    "        print(state,reward,is_done)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0a04747",
   "metadata": {},
   "source": [
    "## Assignment 2 - Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "de72abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['A_U','A_D','A_L', 'A_R'] # set of actions \n",
    "state_reward = [-0.01, -0.01,-0.01,1,-0.01,-0.01,-0.01,-1,-0.01,-0.01,-0.01,-0.01] # reward at every state\n",
    "states = env1.get_all_states() # set of all states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ea18ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Converting the dictionery transition_probs to matrix form\n",
    "transition_matrix = []\n",
    "\n",
    "for s in states:   \n",
    "    x = transition_probs[s]\n",
    "    # print(x)\n",
    "    action_matrix = []\n",
    "    if not bool(x):\n",
    "        # print(x)\n",
    "        transition_matrix.append(np.zeros((len(actions), len(states))))\n",
    "    else:\n",
    "        for a in actions:\n",
    "            y = x[a]\n",
    "            # print(y)\n",
    "            action_state_matrix = []\n",
    "            for i in states:\n",
    "                if i in y.keys():\n",
    "                    action_state_matrix.append(y[i])\n",
    "                else:\n",
    "                    action_state_matrix.append(0)\n",
    "            # print(action_state_matrix)\n",
    "            action_matrix.append(action_state_matrix)\n",
    "        transition_matrix.append(action_matrix)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "08d19e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.9, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0.1, 0.1, 0, 0, 0.8, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0.9, 0, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0.1, 0.8, 0, 0, 0.1, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0.8, 0.2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0.2, 0.8, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " [[0, 0.1, 0.8, 0.1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0.1, 0, 0.1, 0, 0, 0.8, 0, 0, 0, 0, 0],\n",
       "  [0, 0.8, 0.1, 0, 0, 0, 0.1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0.1, 0.8, 0, 0, 0.1, 0, 0, 0, 0, 0]],\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " [[0.8, 0, 0, 0, 0.2, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0.2, 0, 0, 0, 0.8, 0, 0, 0],\n",
       "  [0.1, 0, 0, 0, 0.8, 0, 0, 0, 0.1, 0, 0, 0],\n",
       "  [0.1, 0, 0, 0, 0.8, 0, 0, 0, 0.1, 0, 0, 0]],\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " [[0, 0, 0.8, 0, 0, 0, 0.1, 0.1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0.1, 0.1, 0, 0, 0.8, 0],\n",
       "  [0, 0, 0.1, 0, 0, 0, 0.8, 0, 0, 0, 0.1, 0],\n",
       "  [0, 0, 0.1, 0, 0, 0, 0, 0.8, 0, 0, 0.1, 0]],\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " [[0, 0, 0, 0, 0.8, 0, 0, 0, 0.1, 0.1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0.9, 0.1, 0, 0],\n",
       "  [0, 0, 0, 0, 0.1, 0, 0, 0, 0.9, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0.1, 0, 0, 0, 0.1, 0.8, 0, 0]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0.8, 0.2, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2, 0.8, 0]],\n",
       " [[0, 0, 0, 0, 0, 0, 0.8, 0, 0, 0.1, 0, 0.1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.8, 0.1],\n",
       "  [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0.8, 0.1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.1, 0.8]],\n",
       " [[0, 0, 0, 0, 0, 0, 0, 0.8, 0, 0, 0.1, 0.1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1, 0.9],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0.8, 0.1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0.9]]]"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a18aa72d",
   "metadata": {},
   "source": [
    "### 1. Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "2b4e54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_iter = 10000 \n",
    "value_iter = 10000 \n",
    "\n",
    "p = ['A_L' for s in states] # initilaising a policy \n",
    "V = [0 for s in states] # initialising V0\n",
    "\n",
    "gamma = 1\n",
    "epsilon = 1e-500 # error margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "51a25a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_R', 'A_R', 'A_R', ' ', 'A_U', ' ', 'A_L', ' ', 'A_U', 'A_L', 'A_L', 'A_D']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, policy_iter):\n",
    "    flag = True\n",
    "\n",
    "    # Policy evaluation - computing v(pi)\n",
    "    for j in range(0, value_iter):\n",
    "        m = 0 \n",
    "        V1 = np.zeros(len(states))\n",
    "\n",
    "        for s in range(0, len(states)):\n",
    "            v = state_reward[s] \n",
    "            for s1 in range(0, len(states)):\n",
    "                v = v + transition_matrix[s][actions.index(p[s])][s1] * (gamma * V[s1])\n",
    "\n",
    "            if v - V[s] > m:\n",
    "                m = v - V[s]\n",
    "\n",
    "            V[s] = v  \n",
    "\n",
    "        if m < epsilon:\n",
    "            break\n",
    "\n",
    "    # Policy improvement\n",
    "    for s in range(0, len(states)):\n",
    "\n",
    "        max_v = V[s]\n",
    "        for a in range(0, len(actions)):\n",
    "            v = state_reward[s] \n",
    "            for s1 in range(0, len(states)):\n",
    "                v = v + transition_matrix[s][a][s1] * (gamma * V[s1]) \n",
    "\n",
    "            if v > max_v and actions.index(p[s]) != a:\n",
    "                p[s] = actions[a]\n",
    "                max_v = v\n",
    "                flag = False\n",
    "\n",
    "    if flag:\n",
    "        # print(p)\n",
    "        break\n",
    "\n",
    "for s in states:\n",
    "    if env1.is_terminal(s):\n",
    "        p[states.index(s)] = ' '\n",
    "\n",
    "print(p) # Printing the policy obtained by  policy evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25702cde",
   "metadata": {},
   "source": [
    "### 2. Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "dd30a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 10000 # max number of value iterations\n",
    "epsilon = 1e-500 # error margin \n",
    "\n",
    "p = ['A_L' for s in states] # initialising some policy p\n",
    "V = [0 for s in states] # initilising V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "d77371f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_R', 'A_R', 'A_R', ' ', 'A_U', ' ', 'A_L', ' ', 'A_U', 'A_L', 'A_L', 'A_D']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, iter):\n",
    "    m = 0  \n",
    "    V1 = np.zeros(len(states))  \n",
    "    for s in range(0, len(states)):\n",
    "        max_v = 0\n",
    "        for a in range(0, len(actions)):\n",
    "\n",
    "            v = state_reward[s]\n",
    "            for s1 in range(0, len(states)):\n",
    "                v = v + transition_matrix[s][a][s1] * (gamma * V[s1])\n",
    "\n",
    "            if v > max_v:\n",
    "                max_v = v\n",
    "            \n",
    "            if V[s] >= v:\n",
    "                continue\n",
    "            \n",
    "            p[s] = actions[a] # Updating policy\n",
    "\n",
    "        V1[s] = max_v\n",
    "\n",
    "        if V[s] - V1[s] > m:\n",
    "            m = V[s] - V1[s]\n",
    "\n",
    "    V = V1\n",
    "\n",
    "    if m < epsilon:\n",
    "        # print(p)\n",
    "        break\n",
    "\n",
    "for s in states:\n",
    "    if env1.is_terminal(s):\n",
    "        p[states.index(s)] = ' '\n",
    "\n",
    "print(p) # Printing the policy obtained by  value iteration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4580f645",
   "metadata": {},
   "source": [
    " The policy obtained by policy iteration is :  \n",
    " ['A_R', 'A_R', 'A_R', ' ', 'A_U', ' ', 'A_L', ' ', 'A_U', 'A_L', 'A_L', 'A_D']\n",
    "\n",
    " And \n",
    " The policy obtainde by value iteration is:  \n",
    " ['A_R', 'A_R', 'A_R', ' ', 'A_U', ' ', 'A_L', ' ', 'A_U', 'A_L', 'A_L', 'A_D']\n",
    " \n",
    " Both value iteration (VI) and policy iteration (PI) algorithms converge to the policy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
